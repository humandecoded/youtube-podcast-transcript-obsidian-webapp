# Web app
FLASK_SECRET_KEY=
PORT=5050

# Redis / RQ
REDIS_URL=redis://redis:6379/0
RQ_QUEUE=yt
RQ_JOB_TIMEOUT=3600
RQ_RESULT_TTL=86400
RQ_FAILURE_TTL=604800

# Obsidian
# IMPORTANT: The container writes to /vault; compose will bind-mount your host path there.
OBSIDIAN_VAULT=
OBSIDIAN_FOLDER=
YT_LANGS=en,en-US,en-GB

# Ollama
# If Ollama runs on the HOST at port 11434:
# - macOS/Windows: http://host.docker.internal:11434
# - Linux: compose adds an extra_hosts mapping to resolve host.docker.internal
OLLAMA_BASE_URL=http://host.docker.internal:11434
OLLAMA_MODEL=gpt-oss:120b
OLLAMA_CONTEXT_LENGTH=32000

# ytdlp cookies file path (optional)
YTDLP_COOKIES=

# ytdlp HTTP proxy list file (optional) - text file with one proxy per line
YTDLP_PROXY_FILE=

# Podcast ASR (Automatic Speech Recognition) Configuration
# Enable ASR transcription for podcasts (required for podcast processing)
PODCAST_ASR_ENABLE=1
# Whisper model size: tiny, base, small, medium, large-v1, large-v2, large-v3
# Larger models are more accurate but slower and require more memory
PODCAST_ASR_MODEL=medium.en
# Device to run ASR on: cpu or cuda (CUDA requires compatible GPU)
PODCAST_ASR_DEVICE=cpu
# Compute type: int8 (fastest, cpu), float16 (GPU), float32 (highest quality, slowest)
PODCAST_ASR_COMPUTE=int8# Batch size for transcription: 1 (no batching, CPU), 8-24 (GPU batching for speed)
# Higher values = faster on GPU but require more VRAM. Use 1 for CPU.
PODCAST_ASR_BATCH_SIZE=1

# CPU (safe default)
#PODCAST_ASR_BATCH_SIZE=1

# GPU with 8GB VRAM
#PODCAST_ASR_DEVICE=cuda
#PODCAST_ASR_COMPUTE=float16
#PODCAST_ASR_BATCH_SIZE=16

# GPU with 16GB+ VRAM
#PODCAST_ASR_DEVICE=cuda
#PODCAST_ASR_COMPUTE=float16
#PODCAST_ASR_BATCH_SIZE=24